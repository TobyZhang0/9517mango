{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 导入必要的库\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "\n",
    "# 添加项目根目录到路径中，确保可以导入自定义模块\n",
    "sys.path.append('d:/document/unsw/9517/9517mango')\n",
    "from dataset.mango_dataset import MangoTestDataset  # 假设有这个类，根据实际情况修改\n"
   ],
   "id": "a5fa0bdba8384ecc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n"
   ],
   "id": "fcafcf1818aecd5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 加载测试数据集\n",
    "def load_test_dataset(batch_size=32):\n",
    "    # 根据项目实际情况调整数据加载方式\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    test_dataset = MangoTestDataset(transform=transform)  # 根据实际项目修改\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"测试数据集加载完成，共 {len(test_dataset)} 个样本\")\n",
    "    return test_loader\n"
   ],
   "id": "68db030dacbadafe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 加载所有模型\n",
    "def load_models(models_dir='../models'):\n",
    "    models = {}\n",
    "    \n",
    "    for model_file in os.listdir(models_dir):\n",
    "        model_path = os.path.join(models_dir, model_file)\n",
    "        model_name = os.path.splitext(model_file)[0]\n",
    "        \n",
    "        try:\n",
    "            # 尝试加载PyTorch模型\n",
    "            if model_file.endswith('.pth') or model_file.endswith('.pt'):\n",
    "                model = torch.load(model_path, map_location=device)\n",
    "                model.eval()  # 设置为评估模式\n",
    "                models[model_name] = {'model': model, 'type': 'pytorch'}\n",
    "                \n",
    "            # 尝试加载sklearn模型\n",
    "            elif model_file.endswith('.pkl') or model_file.endswith('.joblib'):\n",
    "                with open(model_path, 'rb') as f:\n",
    "                    model = pickle.load(f)\n",
    "                models[model_name] = {'model': model, 'type': 'sklearn'}\n",
    "                \n",
    "            print(f\"成功加载模型: {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"加载模型 {model_name} 时出错: {e}\")\n",
    "    \n",
    "    return models\n"
   ],
   "id": "6bb5aba0c5e15501"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 评估PyTorch模型\n",
    "def evaluate_pytorch_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return calculate_metrics(all_labels, all_predictions)\n"
   ],
   "id": "43373b39b4aa961c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 评估sklearn模型\n",
    "def evaluate_sklearn_model(model, test_loader):\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # 收集所有数据后再进行预测\n",
    "    all_features = []\n",
    "    \n",
    "    for inputs, labels in test_loader:\n",
    "        # 对于sklearn模型，需要将输入调整为合适的格式\n",
    "        features = inputs.view(inputs.size(0), -1).cpu().numpy()  # 展平为2D数组\n",
    "        all_features.append(features)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_features = np.vstack(all_features)\n",
    "    all_predictions = model.predict(all_features)\n",
    "    \n",
    "    return calculate_metrics(all_labels, all_predictions)\n"
   ],
   "id": "42a9cf032eadf498"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 计算评估指标\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n"
   ],
   "id": "ed846ceada858347"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 主函数\n",
    "def main():\n",
    "    # 加载测试数据集\n",
    "    test_loader = load_test_dataset()\n",
    "    \n",
    "    # 加载所有模型\n",
    "    models = load_models()\n",
    "    print(f\"共加载 {len(models)} 个模型进行评估\")\n",
    "    \n",
    "    # 评估结果存储\n",
    "    results = []\n",
    "    \n",
    "    # 对每个模型进行评估\n",
    "    for model_name, model_info in models.items():\n",
    "        print(f\"正在评估模型: {model_name}\")\n",
    "        model = model_info['model']\n",
    "        model_type = model_info['type']\n",
    "        \n",
    "        if model_type == 'pytorch':\n",
    "            metrics = evaluate_pytorch_model(model, test_loader, device)\n",
    "        else:  # sklearn模型\n",
    "            metrics = evaluate_sklearn_model(model, test_loader)\n",
    "            \n",
    "        metrics['model'] = model_name\n",
    "        results.append(metrics)\n",
    "    \n",
    "    # 创建结果数据框\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # 重新排序列，使模型名称显示在第一列\n",
    "    results_df = results_df[['model', 'accuracy', 'precision', 'recall', 'f1_score']]\n",
    "    \n",
    "    # 结果表格排序（按准确率降序）\n",
    "    results_df = results_df.sort_values(by='accuracy', ascending=False)\n",
    "    \n",
    "    # 显示结果表格\n",
    "    print(\"\\n模型评估结果:\")\n",
    "    display(results_df.style.format({\n",
    "        'accuracy': '{:.4f}',\n",
    "        'precision': '{:.4f}',\n",
    "        'recall': '{:.4f}',\n",
    "        'f1_score': '{:.4f}'\n",
    "    }))\n",
    "    \n",
    "    # 可视化结果\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # 条形图比较各指标\n",
    "    plt.subplot(2, 1, 1)\n",
    "    results_melted = pd.melt(results_df, id_vars=['model'], \n",
    "                            value_vars=['accuracy', 'precision', 'recall', 'f1_score'],\n",
    "                            var_name='指标', value_name='分数')\n",
    "    sns.barplot(x='model', y='分数', hue='指标', data=results_melted)\n",
    "    plt.title('各模型评估指标比较')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 热力图\n",
    "    plt.subplot(2, 1, 2)\n",
    "    sns.heatmap(results_df.set_index('model')[['accuracy', 'precision', 'recall', 'f1_score']], \n",
    "                annot=True, cmap='YlGnBu', fmt='.4f')\n",
    "    plt.title('模型性能热力图')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df\n"
   ],
   "id": "4e51bfc0e8bca234"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 执行主函数\n",
    "results_table = main()\n"
   ],
   "id": "747d29da71344734"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 保存结果到CSV文件\n",
    "results_table.to_csv('../results/model_evaluation_results.csv', index=False)\n",
    "print(\"评估结果已保存到CSV文件\")\n"
   ],
   "id": "1e8c2fb6a850edf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
